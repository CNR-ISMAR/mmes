%% Generated by Sphinx.
\def\sphinxdocclass{report}
\documentclass[letterpaper,10pt,english]{sphinxmanual}
\ifdefined\pdfpxdimen
   \let\sphinxpxdimen\pdfpxdimen\else\newdimen\sphinxpxdimen
\fi \sphinxpxdimen=.75bp\relax

\PassOptionsToPackage{warn}{textcomp}
\usepackage[utf8]{inputenc}
\ifdefined\DeclareUnicodeCharacter
% support both utf8 and utf8x syntaxes
\edef\sphinxdqmaybe{\ifdefined\DeclareUnicodeCharacterAsOptional\string"\fi}
  \DeclareUnicodeCharacter{\sphinxdqmaybe00A0}{\nobreakspace}
  \DeclareUnicodeCharacter{\sphinxdqmaybe2500}{\sphinxunichar{2500}}
  \DeclareUnicodeCharacter{\sphinxdqmaybe2502}{\sphinxunichar{2502}}
  \DeclareUnicodeCharacter{\sphinxdqmaybe2514}{\sphinxunichar{2514}}
  \DeclareUnicodeCharacter{\sphinxdqmaybe251C}{\sphinxunichar{251C}}
  \DeclareUnicodeCharacter{\sphinxdqmaybe2572}{\textbackslash}
\fi
\usepackage{cmap}
\usepackage[T1]{fontenc}
\usepackage{amsmath,amssymb,amstext}
\usepackage{babel}
\usepackage{times}
\usepackage[Bjarne]{fncychap}
\usepackage{sphinx}

\fvset{fontsize=\small}
\usepackage{geometry}

% Include hyperref last.
\usepackage{hyperref}
% Fix anchor placement for figures with captions.
\usepackage{hypcap}% it must be loaded after hyperref.
% Set up styles of URL: it should be placed after hyperref.
\urlstyle{same}
\addto\captionsenglish{\renewcommand{\contentsname}{Contents:}}

\addto\captionsenglish{\renewcommand{\figurename}{Fig.\@ }}
\makeatletter
\def\fnum@figure{\figurename\thefigure{}}
\makeatother
\addto\captionsenglish{\renewcommand{\tablename}{Table }}
\makeatletter
\def\fnum@table{\tablename\thetable{}}
\makeatother
\addto\captionsenglish{\renewcommand{\literalblockname}{Listing}}

\addto\captionsenglish{\renewcommand{\literalblockcontinuedname}{continued from previous page}}
\addto\captionsenglish{\renewcommand{\literalblockcontinuesname}{continues on next page}}
\addto\captionsenglish{\renewcommand{\sphinxnonalphabeticalgroupname}{Non-alphabetical}}
\addto\captionsenglish{\renewcommand{\sphinxsymbolsname}{Symbols}}
\addto\captionsenglish{\renewcommand{\sphinxnumbersname}{Numbers}}

\addto\extrasenglish{\def\pageautorefname{page}}

\setcounter{tocdepth}{1}



\title{MMES Documentation}
\date{Jan 25, 2022}
\release{1.0}
\author{Amedeo Fadini - CNR ISMAR}
\newcommand{\sphinxlogo}{\vbox{}}
\renewcommand{\releasename}{Release}
\makeindex
\begin{document}

\pagestyle{empty}
\sphinxmaketitle
\pagestyle{plain}
\sphinxtableofcontents
\pagestyle{normal}
\phantomsection\label{\detokenize{index::doc}}



\chapter{Introduction}
\label{\detokenize{introduction:introduction}}\label{\detokenize{introduction:id1}}\label{\detokenize{introduction::doc}}
Multi model ensemble system (MMES)

This software is intended to collect daily oceanographic forecasts from different sources and create a Multi-model Ensemble. The software is specifically designed to publish NetCDF files about sea-level and waves, but can be configured to manage other variables.
\begin{quote}

\begin{figure}[H]
\centering

\noindent\sphinxincludegraphics{{multi-model}.png}
\end{figure}
\end{quote}

Ocean forecast results are collected by the system every day in the morning: the program contacts each provider of the list, checks if an updated model exists, downloads it and stores it on a local filesystem using one folder for each node with current and historical data. If the updated forecast is not present in the node, the system will pass to the next node and retry later. Once all forecasts available are downloaded, the multi-model builder prepares the data harmonizing all different forecasts. The ensemble creation procedure can be three main task:
1. retrieve and download each single forecast file provided from different sources;
2. process each forecast with appropriate operations;
3. create the ensemble with mean value and estimate error as standard deviation value and archive old ensemble.
At the present stage, the download and processing tasks are executed in sequence for each source and the cycle is repeated at specified intervals, including new available resources. The choice of Python scripts instead of bash shells procedure allows to execute more than one task in parallel shortening the time of the whole cycle. The software is capable of downloading data from ftp or http/https servers using credentials stored in the configuration files or using a custom command to be executed on the system shell. The software can retrieve virtually any kind of source.

For more information on the ensemble process see:

Ferrarin, C., Valentini, A., Vodopivec, M., Klaric, D., Massaro, G., Bajo, M., De Pascalis, F., Fadini, A., Ghezzo, M., Menegon, S., Bressan, L., Unguendoli, S., Fettich, A., Jerman, J., Ličer, M., Fustar, L., Papa, A., and Carraro, E.:
Integrated sea storm management strategy: the 29 October 2018 event in the Adriatic Sea, Nat. Hazards Earth Syst. Sci., 20, 73\textendash{}93, \sphinxhref{https://doi.org/10.5194/nhess-20-73-2020}{doi: 10.5194/nhess-20-73-2020, 2020}.


\chapter{Software description}
\label{\detokenize{howitworks/index:software-description}}\label{\detokenize{howitworks/index:howitworks}}\label{\detokenize{howitworks/index::doc}}

\section{Inputs}
\label{\detokenize{howitworks/index:inputs}}
To run the MMES software you need to provide
\begin{itemize}
\item {} 
a list of sources sources.json: each source can provide one or more models for one or more variable

\item {} 
a mask file with the final grid you want to get

\item {} 
{[}optional{]} a wheights file for each model that needs to be spatially interpolated to fit the multi-model ansemble

\item {} 
a small amount of configuration information (directory in which all data will be available, variable to be considered …) see {[}{\hyperref[\detokenize{configuration/index:gen-config}]{\sphinxcrossref{\DUrole{std,std-ref}{General Configuration}}}}{]} section

\end{itemize}


\section{Directory structure}
\label{\detokenize{howitworks/index:directory-structure}}\label{\detokenize{howitworks/index:dir-structure}}
The application directory has python scripts and a subdirectory ‘’’scripts’’’ with bash scripts used to download special sources or called by subprocess during program execution

The data directory should have the following structure:

\begin{DUlineblock}{0em}
\item[] data
\item[] ├── MMES
\item[] │   ├── history
\item[] ├── forecasts
\item[] │   ├── SOURCE1
\item[] │   ├── SOURCE2
\item[] │   ├── SOURCE3
\item[] │   ├── …
\item[] ├── mmes\_components
\item[] │   ├── 20211001
\item[] │   ├── 20211002
\item[] │   ├── 20211003
\item[] │   ├── …
\item[] ├── config
\item[] │   ├── mask
\item[] │   ├── wheights
\item[] ├── tmp
\end{DUlineblock}

Inside MMES directory will be placed the daily ensemble produced and inside the MMES/history directory will be stored the old ensamble cutted to first 24 hour
The name of the ensemble can be setted in \sphinxguilabel{config.json} file, thus the MMES directory will take the enemble name instead.

If not already presente the directory structure is automatically created by the management command:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{python} \PYG{n}{manage}\PYG{o}{.}\PYG{n}{py} \PYG{n+nb}{dir}
\end{sphinxVerbatim}


\section{1. Retrieve and download phase}
\label{\detokenize{howitworks/index:retrieve-and-download-phase}}\begin{quote}

\begin{figure}[H]
\centering

\noindent\sphinxincludegraphics{{MMES_download}.png}
\end{figure}
\end{quote}

The source forecasts are provided in different formats and from different types of sources (ftp or http or other). The diagram of the download stage of MMES software is shown in Figure 1. The Daily forecasts are usually published in the morning, but they are not available at the same time and therefore the software contacts all source nodes at regular intervals and checks if the current file is available. If the file is already downloaded and processed the software will pass to the next node.
For http sources the exact path of the file to download is needed, for ftp sources the software needs the directory name and filename. The naming schema of the files is different for each provider but usually can be constructed using a constant pattern and current date value.

If the download process is interrupted due to network issues or other causes, the file can be incomplete and not suitable to create the ensemble. The forecast duration is checked after download (start time, end time) to ensure that it covers at least the time period of the ensemble (2 days), otherwise the file is deleted so it can be downloaded on the next cycle. If the download process is interrupted due to network issues or other causes, the file can be incomplete and not suitable to create the ensemble. The forecast duration is checked after download (start time, end time) to ensure that it covers at least the time period of the ensemble (2 days), otherwise the file is deleted so it can be downloaded on the next cycle.


\section{2. Processing phase}
\label{\detokenize{howitworks/index:processing-phase}}\begin{quote}

\begin{figure}[H]
\centering

\noindent\sphinxincludegraphics{{MMES_processing}.png}
\end{figure}
\end{quote}

If the file is valid, the software will pass to the processing phase. The diagram of the processing stage of MMES software is shown in Figure 2. Each forecast has to be processed in a different way: all the possible steps are implemented in the code (e.g. merge or split variables, rename variables, spatial interpolation on the final grid, temporal interpolation, add tide and offset for sea level, invert wave direction and so on). The processing steps and relative parameters required for each forecast are declared in the configuration files as a JSON object.
On each step, a temporary output file is created: the Python cdo wrapper library manages temporary filenames and makes available the data as a Python variable, then clears all temporary files at the end. Then the result is saved as a NetCDF file inside the component’s directory. At the end of each processing cycle, the software goes to the ensemble creation phase.
The configuration of different processing steps for each variable is the most important part of configuration. See {\hyperref[\detokenize{configuration/index:proc-config}]{\sphinxcrossref{\DUrole{std,std-ref}{Processing steps Configuration}}}} for details.


\section{3. Ensemble creation phase}
\label{\detokenize{howitworks/index:ensemble-creation-phase}}\begin{quote}

\begin{figure}[H]
\centering

\noindent\sphinxincludegraphics[scale=0.8]{{MMES_creation}.png}
\end{figure}
\end{quote}

The general configuration sets a minimum number of files for the ensemble creation: the ensemble output is overwritten on the next cycles adding more forecasts, when available (last execution is scheduled at 14.00). The diagram of the ensemble precaution stage of MMES software is shown in Figure 3.
All numerical model results are interpolated, through a distance-weighted average remapping of the nearest neighbours, on a common regular lat-lon grid covering the Adriatic Sea with a resolution of 0.02 deg.
For coastal flooding hazard purposes, the total sea level height must be forecasted. Therefore, the astronomical tidal level values obtained by a specific SHYFEM application over the Mediterranean Sea (Ferrarin et al., 2018) are added to the residual sea level simulated by the operational systems not accounting for the tide (e.g. SHYMED, ISSOS). The obtained sea level heights simulated by the different models are all referred to the geoid.

Figure 3: Diagram of ensemble creation stage of MMES software.
The CDO library provides simple commands to compute the mean and standard deviation of a variable. For the wave ensemble we have three different variables, wave significant height, wave period and wave direction: the wave direction is expressed in degrees and must be splitted in the U and V components, then merge the ensembles again.
The ensemble forecast duration is 2 days with 48 hourly timesteps, but users can set a different duration in configuration files. When the new ensemble is ready, the previous day is trimmed to the first 24h hours and archived in the history folder: the Thredds data server will publish the whole collection so can be downloaded a subset of custom duration for the past multi-model files.


\section{MMES Outputs}
\label{\detokenize{howitworks/index:mmes-outputs}}
MMES produces 2-day (duration is defined in \sphinxhref{https://github.com/CNR-ISMAR/mmes/blob/8f17dd136ef268e18359769633fb032bd3dd4c62/mmes\_functions.py\#L95}{mes\_functions.py L95} )probabilistic forecasts in terms of the ensemble \sphinxguilabel{mean} and \sphinxguilabel{standard} deviation for both the sea level height and wave over the whole Adriatic Sea and part of the Ionian Sea. The spread (i.e. standard deviation) among the operational simulations is expected to represent a measure of the uncertainty of the prediction and should be linked to the forecast error so that cases with the largest spread are those with the highest uncertainty and where a large error of the ensemble mean (and also of the deterministic forecast) is more likely (Flowerdew et al., 2010).
It is not straightforward what averaging weights should be used for the multi-model ensemble forecast and therefore we used equally weighted ensemble members, despite the forecasts which are more precise than others should have more importance in the MMES (Salighehdar et al., 2017; Schevenhoven and Selten, 2017). Here we applied a simple average of the forecasts at every timestamp to compute the ensemble mean, but more sophisticated methods based on weighting function determined by comparison of the single model results with near real-time observations will be implemented in future (Di Liberto et al., 2011; Salighehdar et al., 2017). Taking advantage of the near real-time observations acquired by the aggregated monitoring network, the root mean square error of the individual forecast will be next evaluated and stored for long-term statistics.
MMES forecasts are produced each day. MMES outputs (in terms of ensemble mean and standard deviation of the sea level and waves) in NetCDF format are available to the end-users and external portals through the CNR-ISMAR Thredds Data Server at the webpage’s url \sphinxurl{https://iws.ismar.cnr.it/thredds/catalog/tmes/catalog.html}. The results of the multi-model ensemble system can be visualized via the I-STORMS Geoportal web interfaces (\sphinxurl{https://iws.seastorms.eu/}). The results will be next delivered through the STREAM International Flood Platform.


\chapter{Installation}
\label{\detokenize{setup/index:installation}}\label{\detokenize{setup/index::doc}}
The software i sintended to run aona GNU/linux based server
\begin{enumerate}
\def\theenumi{\arabic{enumi}}
\def\labelenumi{\theenumi .}
\makeatletter\def\p@enumii{\p@enumi \theenumi .}\makeatother
\item {} 
Create a python \sphinxhref{https://docs.python.org/3/library/venv.html}{virtualenv} and activate it.

\end{enumerate}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{python3} \PYG{o}{\PYGZhy{}}\PYG{n}{m} \PYG{n}{venv} \PYG{o}{/}\PYG{n}{path}\PYG{o}{/}\PYG{n}{to}\PYG{o}{/}\PYG{n}{new}\PYG{o}{/}\PYG{n}{virtual}\PYG{o}{/}\PYG{n}{environment}\PYG{o}{/}\PYG{n}{mmes}
\PYG{n}{source} \PYG{n}{activate} \PYG{o}{/}\PYG{n}{path}\PYG{o}{/}\PYG{n}{to}\PYG{o}{/}\PYG{n}{new}\PYG{o}{/}\PYG{n}{virtual}\PYG{o}{/}\PYG{n}{environment}\PYG{o}{/}\PYG{n}{mmes}\PYG{o}{/}\PYG{n+nb}{bin}\PYG{o}{/}\PYG{n}{activate}
\end{sphinxVerbatim}
\begin{enumerate}
\def\theenumi{\arabic{enumi}}
\def\labelenumi{\theenumi .}
\makeatletter\def\p@enumii{\p@enumi \theenumi .}\makeatother
\setcounter{enumi}{1}
\item {} 
Clone this repository in a convenient location.

\end{enumerate}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{git} \PYG{n}{clone} \PYG{n}{git}\PYG{n+nd}{@github}\PYG{o}{.}\PYG{n}{com}\PYG{p}{:}\PYG{n}{CNR}\PYG{o}{\PYGZhy{}}\PYG{n}{ISMAR}\PYG{o}{/}\PYG{n}{mmes}\PYG{o}{.}\PYG{n}{git}
\end{sphinxVerbatim}
\begin{enumerate}
\def\theenumi{\arabic{enumi}}
\def\labelenumi{\theenumi .}
\makeatletter\def\p@enumii{\p@enumi \theenumi .}\makeatother
\setcounter{enumi}{2}
\item {} 
Install requirements with

\end{enumerate}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{pip} \PYG{n}{install} \PYG{o}{\PYGZhy{}}\PYG{n}{r} \PYG{n}{requirements}\PYG{o}{.}\PYG{n}{txt}
\end{sphinxVerbatim}
\begin{enumerate}
\def\theenumi{\arabic{enumi}}
\def\labelenumi{\theenumi .}
\makeatletter\def\p@enumii{\p@enumi \theenumi .}\makeatother
\setcounter{enumi}{3}
\item {} 
Prepare the data directory structure according to the {\hyperref[\detokenize{howitworks/index:dir-structure}]{\sphinxcrossref{\DUrole{std,std-ref}{Directory structure}}}} section.

\end{enumerate}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{python} \PYG{n}{manage}\PYG{o}{.}\PYG{n}{py} \PYG{n+nb}{dir}
\end{sphinxVerbatim}
\begin{enumerate}
\def\theenumi{\arabic{enumi}}
\def\labelenumi{\theenumi .}
\makeatletter\def\p@enumii{\p@enumi \theenumi .}\makeatother
\setcounter{enumi}{4}
\item {} 
Manually edit the general configuration file \sphinxguilabel{config.json} according to your needs (see {\hyperref[\detokenize{configuration/index:gen-config}]{\sphinxcrossref{\DUrole{std,std-ref}{General Configuration}}}}) section.

\item {} 
From the main directory launch

\end{enumerate}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{python} \PYG{n}{manage}\PYG{o}{.}\PYG{n}{py} \PYG{n}{new}
\end{sphinxVerbatim}

to create a new source config file or

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{python} \PYG{n}{manage}\PYG{o}{.}\PYG{n}{py} \PYG{n}{mod}
\end{sphinxVerbatim}

to edit existing source config file.
Refer to {\hyperref[\detokenize{configuration/index:configuration}]{\sphinxcrossref{\DUrole{std,std-ref}{Configuration files}}}} section fro detailed information about config files
\begin{enumerate}
\def\theenumi{\arabic{enumi}}
\def\labelenumi{\theenumi .}
\makeatletter\def\p@enumii{\p@enumi \theenumi .}\makeatother
\setcounter{enumi}{6}
\item {} 
For each source forecast  you need to add the model name to required step in \sphinxguilabel{processing.json} {\hyperref[\detokenize{configuration/index:proc-config}]{\sphinxcrossref{\DUrole{std,std-ref}{Processing steps Configuration}}}}

\end{enumerate}


\chapter{Configuration files}
\label{\detokenize{configuration/index:configuration-files}}\label{\detokenize{configuration/index:configuration}}\label{\detokenize{configuration/index::doc}}

\section{General Configuration}
\label{\detokenize{configuration/index:general-configuration}}\label{\detokenize{configuration/index:gen-config}}
General configuration directives are stored in:guilabel:\sphinxtitleref{config.json} file
The file has the following directives:
\begin{itemize}
\item {} 
\sphinxstylestrong{data\_dir} root of your data dir see {\hyperref[\detokenize{howitworks/index:dir-structure}]{\sphinxcrossref{\DUrole{std,std-ref}{Directory structure}}}}

\item {} 
\sphinxstylestrong{sources\_file} name of sources config file (default: \sphinxstyleemphasis{sources.json})

\item {} 
\sphinxstylestrong{ensemble\_name} name of the enemble yo’re going to create, this name will used for filenames

\item {} 
\sphinxstylestrong{ensemble\_variables} a JSON objects with the variable of each ensemble file (will be used in filename) and the related array for variable names in the final NetCDF file. Please note that the original variables of each forecast will be renamed following this order according to the processing.json file

\item {} 
\sphinxstylestrong{mask\_file} name of final grid of ensemble, all value outside this mask will be set to nodata

\item {} 
\sphinxstylestrong{gap\_days} max umber of days that will be attempted to be automatically gap-filled: check the output at least every n days

\end{itemize}

Default config.json:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{p}{\PYGZob{}}
\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{data\PYGZus{}dir}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{/usr3/iwsdata}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{sources\PYGZus{}file}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{sources.json}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{ensemble\PYGZus{}name}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{TMES}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{ensemble\PYGZus{}variables}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{p}{\PYGZob{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{sea\PYGZus{}level}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{sea\PYGZus{}level}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}\PYG{p}{,} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{waves}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{whs}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{wmp}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{wmd}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}\PYG{p}{\PYGZcb{}}\PYG{p}{,}
\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{mask\PYGZus{}file}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+si}{\PYGZob{}data\PYGZus{}dir\PYGZcb{}}\PYG{l+s+s2}{/config/mask/TMES\PYGZus{}mask\PYGZus{}002\PYGZus{}ext.nc}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{gap\PYGZus{}days}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:}  \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{5}\PYG{l+s+s2}{\PYGZdq{}}
\PYG{p}{\PYGZcb{}}
\end{sphinxVerbatim}

The config value will be loaded \sphinxstylestrong{recursively} by loadconfig() function so in the file you can use previously declared values between curly brackets such as \{data\_dir\} that will be replaced by the actual \sphinxstylestrong{data\_dir} value.


\section{Suorce Configuration}
\label{\detokenize{configuration/index:suorce-configuration}}\label{\detokenize{configuration/index:src-config}}
Source list are stored by default in the \sphinxguilabel{sources.json} file in the root directory, the filename can be customized in general config \sphinxguilabel{config.json}

Refer to \sphinxguilabel{source\_template.json} to create your own configuration file.

If the ensemble creation for sea\_level include models that need to add tide to express total sea level we suggest to put the tide source in the first place of config file.

You can also run from command line:
\begin{quote}

\sphinxcode{\sphinxupquote{{}`
python manage.py new
{}`}}
\end{quote}

This  will start a \sphinxstyleemphasis{step by step} procedure to create a new sources.json file. If something goes wrong you’ll find a convenient backup of your last configuration on sources.json.bak


\section{Processing steps Configuration}
\label{\detokenize{configuration/index:processing-steps-configuration}}\label{\detokenize{configuration/index:proc-config}}
The processing steps for each model are defined in \sphinxguilabel{processing.json} file.
You can edit the file and add the model system name to tha step of your interest under each variable group

The processing configuration file is composed by two section: \sphinxguilabel{sea\_level\_prepare} and \sphinxguilabel{waves\_prepare}
when modify or adding new models the manage.py script help to add (or remove) the current model to each step.


\subsection{Sea level prepare}
\label{\detokenize{configuration/index:sea-level-prepare}}
In this table are described the steps for sea level prepare


\begin{savenotes}\sphinxattablestart
\centering
\sphinxcapstartof{table}
\sphinxthecaptionisattop
\sphinxcaption{Processing steps sea level}\label{\detokenize{configuration/index:id1}}
\sphinxaftertopcaption
\begin{tabulary}{\linewidth}[t]{|T|T|T|}
\hline
\sphinxstyletheadfamily 
JSON name
&\sphinxstyletheadfamily 
Description
&\sphinxstyletheadfamily 
Can be skipped if…
\\
\hline
variable\_selection
&
Select only sea level variable
&
source model has only sea level variable: original variable name is setted in {\hyperref[\detokenize{configuration/index:src-config}]{\sphinxcrossref{\DUrole{std,std-ref}{Suorce Configuration}}}}
\\
\hline
temporal\_interpolation
&
Temporal interpolation to match start, end and timesteps
&
source model has exact timesteps of the ensemble
\\
\hline
get\_48hours
&
Get first 48hours of forecast model
&
source model is already 48hours long
\\
\hline
add\_factor
&
Add a specific  offset factor (setted in {\hyperref[\detokenize{configuration/index:src-config}]{\sphinxcrossref{\DUrole{std,std-ref}{Suorce Configuration}}}} as \sphinxcode{\sphinxupquote{sea\_level\_fact}}) for this model to match reference level
&
the reference level is the same  of ensemble
\\
\hline
mask\_before\_interpolation
&
Add a mask to some part of the model before interpolation (values setted in {\hyperref[\detokenize{configuration/index:src-config}]{\sphinxcrossref{\DUrole{std,std-ref}{Suorce Configuration}}}})
&
All values of source model are suitable
\\
\hline
spatial\_interpolation
&
Interpolate to match same grid of the ensemble
&
already match same grid of the ensemble
\\
\hline
extrapolate\_missing
&
Extrapolate missing values
&
No need to fill missing value in source model
\\
\hline
mask\_after\_interpolation
&
Add a mask to some part of the model after interpolation (values setted in {\hyperref[\detokenize{configuration/index:src-config}]{\sphinxcrossref{\DUrole{std,std-ref}{Suorce Configuration}}}})
&
All values of source model are suitable
\\
\hline
mask\_outside\_area
&
Mask value outside area of interest
&
The extension of source model is the same of ensemble
\\
\hline
add\_tide
&
Add astronomical tide model  to source forecast model
&
Forecast model already integrte tide.
\\
\hline
\end{tabulary}
\par
\sphinxattableend\end{savenotes}


\subsection{Waves prepare}
\label{\detokenize{configuration/index:waves-prepare}}

\begin{savenotes}\sphinxattablestart
\centering
\sphinxcapstartof{table}
\sphinxthecaptionisattop
\sphinxcaption{Processing steps waves}\label{\detokenize{configuration/index:id2}}
\sphinxaftertopcaption
\begin{tabulary}{\linewidth}[t]{|T|T|T|}
\hline
\sphinxstyletheadfamily 
JSON name
&\sphinxstyletheadfamily 
Description
&\sphinxstyletheadfamily 
Can be skipped if…
\\
\hline
merge\_components
&
Merge multiple files from the source forecast to have allthe waves variables (Wave Significant Height, Wave period and Wave direction) in the same file.
&
Source forecast model already have the three variables in the same file.
\\
\hline
variable\_selection
&
Select from source file only the three variables about waves (variable names setted in {\hyperref[\detokenize{configuration/index:src-config}]{\sphinxcrossref{\DUrole{std,std-ref}{Suorce Configuration}}}}) and rename them according to \sphinxguilabel{ensemble\_variables} in \sphinxguilabel{config.json}.
&
The source file has only the variables about waves and
\\
\hline
invert\_latitude
&
Invert latitude direction usng \sphinxcode{\sphinxupquote{cdo invertlat}} command
&
The latitude is already coherent with the ensemble
\\
\hline
set\_miss\_value
&
Fill missing value with \sphinxcode{\sphinxupquote{cdo setmissval}} . Missing value is setted in {\hyperref[\detokenize{configuration/index:src-config}]{\sphinxcrossref{\DUrole{std,std-ref}{Suorce Configuration}}}} (\sphinxcode{\sphinxupquote{miss\_value}})
&
The source forecast
\\
\hline
change\_int\_float
&
Change data type of variables integer  float
&
Data type for all variables are already float
\\
\hline
temporal\_interpolation
&
Temporal interpolation to match start, end and timesteps
&
source model has exact timesteps of the ensemble
\\
\hline
get\_48hours
&
Get 48hours
&
Step description
\\
\hline
set\_grid\_unstructured
&
Set grid unstructured
&
Step description
\\
\hline
spatial\_interpolation
&
Spatial interpolation
&
Step description
\\
\hline
extrapolate\_missing
&
Extrapolate missing value with \sphinxcode{\sphinxupquote{{}`cdo fillmiss{}`}}
&
Step description
\\
\hline
mask\_after\_interpolation
&
Add a mask to some part of the model after interpolation (values setted in {\hyperref[\detokenize{configuration/index:src-config}]{\sphinxcrossref{\DUrole{std,std-ref}{Suorce Configuration}}}})
&
All values of source model are suitable
\\
\hline
mask\_outside\_area
&
Mask value outside area of interest
&
The extension of source model is the same of ensemble
\\
\hline
remove\_zero\_values
&
Remove values setted to zero and replace with missing value
&
No data values are setted correctly
\\
\hline
\end{tabulary}
\par
\sphinxattableend\end{savenotes}



\renewcommand{\indexname}{Index}
\printindex
\end{document}